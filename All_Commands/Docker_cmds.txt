# To get the permission for docker
sudo chmod 666 /var/run/docker.sock

# docker root directory
Docker Root Dir: /var/lib/docker

# to access the BTNAS003 file
docker run -it --rm --runtime nvidia --network host -v /:/root shri_tf

#---------------- cudNN error, the cuda path is not added ----------
cd /etc/ --> vi bash.bashrc
export LD_LIBRARY_PATH=/usr/local/cuda-11.1/lib64:$LD_LIBRARY_PATH


docker images               #shows all Docker images
docker ps                   #shows all active Docker containers
docker ps -a                # also shows stopped Docker containers
docker image rm IMAGE       # remove one or more images
docker rm IMAGE             # remove (stopped) container
docker rmi IMAGE
docker image rm IMAGE           # remove Docker image
docker image IMAGE_ID       #

#show running processes
ps -eaf

# To mount and run the image "Host dir:container dir"
docker run -it --rm --runtime nvidia --network host -v /nvme/Shrihari:/Shrihari testing
OR
docker run -it --rm -v /nvme/Shrihari:/Shrihari testing1

#show free space on docker partition
df -h /var/lib/docker

#execute command in runnin docker container
docker exec <container-name> <command>
docker exec -d                      #detacked: run in background
docker exec -it                     #run interactively, in pseudo terminal

#show running process in active container
ps -eaf
#run <command> in container
docker exec -d <container-name> <command>  #-d detached mode, -i -t interactively in pseudoshell 

#Remove all unused containers, networks, images (both dangling and unreferenced)
docker system prune

#save and load docker images
docker save -o <path for generated tar file> <image_name>
docker load -i <path to image tar file>

#displays information regarding the amount of disk space used by the docker daemon
docker system df -v

#save new Docker image
docker commit CONTAINERID  NEW_IMAGE_NAME
#The container still has to be active

#Standard command for for running the NGC Tensorflow Docker container
docker run --gpus all --shm-size=1g --ulimit memlock=-1 -it --rm -v /BTNAS003/sck/monodepth/Christian:/Christian 19.12-tf2-py3-nvtx

#advanced command:
docker run --gpus all --shm-size=1g --ulimit memlock=-1 -it --rm -v /mnt/btcfiles01/Users/sck/monodepth_new/Christian:/Christian nvcr.io/nvidia/tensorflow:19.12-tf1-py3_nvtx-cv


docker run --gpus all --shm-size=1g --ulimit memlock=-1 -it --rm -v /mnt/btcfiles01/Users/sck/monodepth_new/Christian:/Christian 19.12-tf1-py3-nvtx-cv

docker run --gpus all --shm-size=10g --ulimit memlock=-1 -it --security-opt seccomp=default_2.json --rm -v /BTNAS003/sck/monodepth/Christian:/Christian 19.12-tf1-py3-nvtx-cv

docker run --gpus all --shm-size=1g --ulimit memlock=-1 --privileged=true -it --rm -v /BTNAS003/sck/monodepth/Christian:/Christian 19.12-tf1-py3-nvtx-cv

docker run --gpus all --shm-size=5g --ulimit memlock=-1 --ulimit stack=67108864 -it --privileged=true --rm -v /BTNAS003/sck/monodepth/Christian:/Christian 19.12-tf1-py3-nvtx-cv

docker run --gpus all --shm-size=5g --ulimit memlock=-1 -it --privileged=true --rm -v /BTNAS003/sck/monodepth/Christian:/Christian -v /usr/src/tensorrt/data/resnet50:/data  20.01-tf1-py3-cv

docker run --gpus all --shm-size=5g --ulimit memlock=-1 -it --privileged=true --rm -v /home/sck/Documents/monodepth/Christian:/Christian -v /usr/src/tensorrt/data/resnet50:/data 20.01-tf1-py3-cv

#ONNX runtime:
docker run --gpus all --shm-size=5g --ulimit memlock=-1 -it --rm -v /home/sck/Documents/monodepth/Christian:/Christian mcr.microsoft.com/azureml/onnxruntime:latest-tensorrt




nsys profile --force-overwrite true -o ./nsys_timeline/timeline_without python ./monodepth_serve_frozen_graph_resize_NVTXHook.py

nsys profile -w true -t 'cudnn,cuda,osrt,nvtx' --force-overwrite true -o ./nsys_timelines/timeline_3json_disabled_flags python ./monodepth_serve_frozen_graph_resize_test.py

nsys profile -w true -t 'cudnn,cuda,osrt,nvtx' --force-overwrite true -o ./nsys_timelines/timeline_NVTXHook_4it_disabled python ./monodepth_serve_frozen_graph_resize_NVTXHook.py

nsys profile -w true -t 'nvtx' --force-overwrite true -o ./nsys_timelines/timeline_privileged_NVTXHook_4it_nvtx python ./monodepth_serve_frozen_graph_resize_NVTXHook.py

nsys profile -y 2 -w true -t 'cudnn,cuda,nvtx' --force-overwrite true -o ./nsys_timelines/timeline_NVTXHook_10it_default3_wo_osrt python ./monodepth_serve_frozen_graph_resize_NVTXHook_2.py

nsys profile -y 2 -w true -t 'cudnn,cuda,nvtx,osrt,cublas' --force-overwrite true -o ./nsys_timelines/tensorrt/timeline_tensorrt_fp16_10it_64batch_container_privileged python ./monodepth_TensorRT_serve_serializedengine_difBatchSize_fp16.py

nsys profile -y 5 -w true -t 'cudnn,cuda,nvtx,osrt,cublas' --force-overwrite true -o ./nsys_timelines/tensorrt_resnet50/timeline_resnet50_tensorrt_10it_2batch_container_privileged python ./resnet50v1_uff_tensorrt_batch_difinput.py

nsys profile -y 2 -w true -t 'cudnn,cuda,nvtx,osrt,cublas' --force-overwrite true -o ./nsys_timelines/tensorrt_resnet50/timeline_resnet50_tensorflow_10it_1batch_container_privileged python ./resnet50v1_serve_frozen_graph_batch_difinput.py -d /data

/usr/src/tensorrt/data/resnet50

Collection of IP samples, backtraces, and scheduling data disabled. perf event paranoid level is 3. 
Change the paranoid level to 2 to enable IP sample, backtrace, and scheduling data collection. Change the paranoid level to 1 to enable kernel sample collection. 
Try
sudo sh -c 'echo [level] >/proc/sys/kernel/perf_event_paranoid'
where 'level' equals 1 or 2 to change the paranoid level.

sudo sh -c 'echo 1 >/proc/sys/kernel/perf_event_paranoid'

nsys status --environment

export TF_DISABLE_NVTX_RANGES=1


--capture-range=nvtx


# run your script 
sudo docker run -it --rm --runtime nvidia --network host -v /media/muttagsi/nvme/Shrihari:/Shrihari nvcr.io/nvidia/l4t-ml:r32.4.3-py3

sudo docker run -it --runtime nvidia --network host -v /nvme/Shrihari:/Shrihari latest_py python3 /Shrihari/test.py

